{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFEyeGazeSharedTask_Features_LSTM_19/20.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"80bHKBp1j9ae"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyBe-0ddKQaE"},"source":["# EYE GAZE SHARED TASK"]},{"cell_type":"code","metadata":{"id":"e8yEHuC4J9S1"},"source":["import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from sklearn.metrics import r2_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOMNpLleKSkN"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rtPkvPN0kJPV"},"source":["### DOWNLOAD DATA"]},{"cell_type":"code","metadata":{"id":"kRxJ5AXBLUy5"},"source":["train_data_file_path = \"/content/drive/My Drive/CMCL Shared Task/preprocessed_dataset.csv\"\n","test_data_file_path = \"/content/drive/My Drive/CMCL Shared Task/preprocessed_test_dataset.csv\"\n","\n","#file_path = \"/content/drive/MyDrive/datasets/EyeGazeSharedTask/trial_data.csv\"\n","\n","train_data = pd.read_csv(train_data_file_path)\n","test_data = pd.read_csv(test_data_file_path)\n","df = train_data.copy()\n","test_df = test_data.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZYNfiti9c-D"},"source":["print(\"Train Data Shape = \", train_data.shape)\n","print(\"Test Data Shape = \", test_data.shape)\n","train_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Ude7qQ6hZxM"},"source":["### BASIC MODEL PLAN"]},{"cell_type":"code","metadata":{"id":"1a6iBuw2Zi3a"},"source":["#@title\n","import zipfile\n","larger_file_path = \"/content/drive/My Drive/CMCL Shared Task/glove.840B.300d.zip\"\n","smaller_file_path = \"/content/drive/My Drive/CMCL Shared Task/glove.6B.200d.txt\"\n","\n","def load300GloveModel(File):\n","    file_path = \"glove.840B.300d.txt\"\n","\n","    print(\"Unzipping File...\")\n","    with zipfile.ZipFile(File, \"r\") as zip:\n","      zip.extractall()\n","    print(\"Finished unzipping File\")\n","    \n","    print(\"Loading Glove Model\")\n","    f = open(file_path,'r')\n","    gloveModel = {}\n","    for line in f:\n","        splitLines = line.split(' ')\n","        try:\n","          word = splitLines[0]\n","          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n","          gloveModel[word] = wordEmbedding\n","        except:\n","          print(\"Error encountered. Skipping word \", word)\n","    print(len(gloveModel),\" words loaded!\")\n","    return gloveModel\n","\n","def load200GloveModel(File):\n","  f = open(File,'r')\n","  gloveModel = {}\n","  for line in f:\n","      splitLines = line.split(' ')\n","      try:\n","        word = splitLines[0]\n","        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n","        gloveModel[word] = wordEmbedding\n","      except:\n","        print(\"Error encountered. Skipping word \", word)\n","  print(len(gloveModel),\" words loaded!\")\n","  return gloveModel\n","\n","#glove_embedding_dict = load300GloveModel(larger_file_path)\n","glove_embedding_dict = load200GloveModel(smaller_file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSMa-WAPh9Q7"},"source":["df_data = pd.read_csv(\"/content/drive/My Drive/CMCL Shared Task/Sentences_maxlen.csv\")\n","sentences = list(df_data[\"sentences\"])\n","MAX_LEN = df_data[\"max_len\"][0]\n","\n","test_df_data = pd.read_csv(\"/content/drive/My Drive/CMCL Shared Task/Test_Sentences_maxlen.csv\")\n","test_sentences = list(test_df_data[\"sentences\"])\n","test_MAX_LEN = test_df_data[\"max_len\"][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6vQxr_IB1Qy"},"source":["VOCAB_SIZE = 400000\n","BATCH_SIZE = 32\n","N_FEATURES = 13\n","N_TARGETS = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBCT2Een7o-Y"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n","tokenizer.fit_on_texts(sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJ4_P7b07gdg"},"source":["word_index = tokenizer.word_index\n","print(len(word_index))\n","embedding_matrix = np.zeros((len(word_index) + 1, 200)) # words not found in embedding index will be all-zeros.\n","for word, i in word_index.items():\n","  embedding_vector = glove_embedding_dict.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWANQgekC2ZB"},"source":["tokens = tokenizer.texts_to_sequences(sentences)\n","test_tokens = tokenizer.texts_to_sequences(test_sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcF8lH4aG3Qz"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","pad_sent = pad_sequences(tokens, maxlen=MAX_LEN)\n","test_pad_sent = pad_sequences(test_tokens, maxlen=MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbjD1vIq2zno"},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### PREPROCESS DATA"]},{"cell_type":"markdown","metadata":{"id":"fYLOLvUm1jOD"},"source":["#### REMOVE EOS TOKEN"]},{"cell_type":"code","metadata":{"id":"J1P_bWCs2baB","cellView":"form"},"source":["#@title\n","# FUNCTION TO REMOVE THE <EOS> TOKEN IN THE DATASET\n","\n","def remove_eos(df):\n","  cnt = 1\n","  endword = []\n","  for i in range(df.shape[0]-1):\n","    if (df.loc[i+1, \"sentence_id\"] == cnt):\n","      df.loc[i, \"word\"] = df.loc[i, \"word\"][:-5]   # Remove <EOS> for the last word of each sentence.\n","      cnt += 1 \n","      endword.append(1)\n","    else:\n","      endword.append(-1)\n","\n","  s = df.loc[df.shape[0] - 1, \"word\"]              # Remove <EOS> for last element separately\n","  df.loc[df.shape[0] - 1, \"word\"] = s[:-6] \n","  endword.append(1)\n","  df[\"endword\"] = endword\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0qirQb-2EE7"},"source":["### NO OF CHARS FOR EACH WORD"]},{"cell_type":"code","metadata":{"id":"JQ4D7kRN1cOB","cellView":"form"},"source":["#@title\n","# FUNCTION TO CALCULATE THE NUMBER OF CHARACTERS PER WORD\n","# ADDS THE DATA IN A NEW COLUMN\n","\n","def char_per_word(df):\n","  n_chars = []\n","  for word in df.word:\n","    n_chars.append(len(str(word)))\n","  df[\"n_chars\"] = n_chars\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJnVAg8QccsI"},"source":["### NO OF CHARS OF WORD - LEMMATIZED WORD"]},{"cell_type":"code","metadata":{"id":"6nVwk1nab9nf","cellView":"form"},"source":["#@title\n","# FUNCTION TO CALCULATE THE DIFFERENCE BETWEEN NUMBER OF CHARACTERS IN WORD AND LEMMATIZED WORD\n","# ADDS AS A NEW COLUMN\n","\n","from nltk import WordNetLemmatizer\n","\n","Lemmatizer = WordNetLemmatizer()\n","\n","def char_per_lemmatized_word(df):\n","  n_chars = []\n","  for word in df.word:\n","    n_chars.append(len(str(word)) - len(Lemmatizer.lemmatize(word)))\n","  df[\"n_char_lemmatized\"] = n_chars\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNya6DDO2KlQ"},"source":["### STOP WORD OR NOT"]},{"cell_type":"code","metadata":{"id":"P9wvuFxh1fKX","cellView":"form"},"source":["#@title\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","\n","stopwords = nltk.corpus.stopwords\n","stop_words = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvOW7IN72NJy","cellView":"form"},"source":["#@title\n","# FUNCTION TO ASSERT WHETHER A WORD IS STOPWORD  OR NOT\n","# ADDS THE DATA IN A NEW COLUMN\n","\n","def add_stopword_check(df):\n","  if_stopword = []\n","  for word in df.word:\n","    if word in stop_words:\n","      if_stopword.append(1)\n","    else:\n","      if_stopword.append(-1)\n","\n","  df[\"stopword\"] = if_stopword\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzMv3ir42Wl4"},"source":["### NUMBER OR NOT"]},{"cell_type":"code","metadata":{"id":"6knkuEi12RAb","cellView":"form"},"source":["#@title\n","# FUNCTION TO DEFINE WHETHER IT IS A NUMBER OR NOT\n","# ADDS THE DATA AS A NEW COLUMN\n","\n","def add_number_check(df):\n","  if_number = []\n","  for word in df.word:\n","    if word.isdigit():\n","      if_number.append(1)\n","    else:\n","      if_number.append(-1)\n","\n","  df[\"number\"] = if_number\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2AGt92OXu81b"},"source":["### TF IDF CALCULATION"]},{"cell_type":"code","metadata":{"id":"b_v29e6ccqag","cellView":"form"},"source":["#@title\n","# FUNCTION TO CALCULATE THE TFIDF OF THE TRAINING DATASET\n","# ADDS THE DATA IN A NEW COLUMN\n","# ALSO RETURNS A LIST OF THE SENTENCES\n","\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(stop_words=None)\n","bad_words = []\n","punc = string.punctuation\n","\n","def remove_punc(word):\n","  table = str.maketrans('', '', punc)\n","  return word.translate(table)\n","\n","def calc_tfidf(df):\n","  n = np.array(df[\"sentence_id\"])[-1]\n","  sentences = []\n","  sentence_tokens = []\n","  tf_idfs = []\n","  MAX_LEN = 0\n","  for i in range(n+1):   \n","    temp_df = df[df.sentence_id == i]\n","    sentence = (' ').join(temp_df.word)\n","    MAX_LEN = max(MAX_LEN, len(sentence))\n","    sentences.append(sentence)\n","    sentence_tokens.append([np.array(temp_df.word)])\n","  tf_idf = vectorizer.fit_transform(sentences)\n","  for i, word in enumerate(df.word):\n","    try:\n","      tf_idfs.append(tf_idf.toarray()[df[\"sentence_id\"][i]][vectorizer.get_feature_names().index(remove_punc(word.lower()))])\n","    except:\n","      bad_words.append(word)\n","      if word in [\"a\", \"A\"]:\n","        tf_idfs.append(0.8)\n","      else:\n","        tf_idfs.append(0.01)\n","  df[\"tf_idf\"] = tf_idfs\n","\n","\n","  return sentence_tokens, df, bad_words, MAX_LEN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uODThGOIrKDA","cellView":"form"},"source":["#@title\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize \n","from sklearn.preprocessing import OneHotEncoder\n","\n","enc = OneHotEncoder(sparse = False)\n","\n","def pos_tag_func(df):\n","  tags = []\n","  for word in df.word:\n","    if word not in string.punctuation:\n","      tag = nltk.pos_tag(word)[0][1]\n","    else:\n","      tag = \"PUNC\"\n","    tags.append(tag)\n","  df[\"tags\"] = tags\n","  tag_transform = pd.DataFrame(enc.fit_transform(np.array(df.tags).reshape(-1, 1)))\n","  df = pd.concat((df, tag_transform), axis = 1)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8IEDwE_rv5i","cellView":"form"},"source":["#@title\n","def use_transformed_GPT(df):\n","  df[\"GPT\"] = df[\"TRT\"] - df[\"GPT\"]\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zww9QlLhxHfw"},"source":["# FUNCTION TO PERFORM ALL PREPROCESSING STEPS\n","\n","def preprocess(df):\n","  return calc_tfidf(use_transformed_GPT(pos_tag_func(add_number_check(add_stopword_check(char_per_lemmatized_word(char_per_word(remove_eos(df))))))))\n","\n","sentence_tokens, df, bad_words, MAX_LEN = preprocess(train_data)\n","\n","df1 = df.copy()\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRJItglAio_t"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7uWHqgmL8vm"},"source":["print(\"No of words unaccounted for = \", len(bad_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kq9lKYwYoLkv"},"source":["# FUNCTION TO NORMALISE ALL THE TARGET VALUES\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","std_scaler = StandardScaler()\n","\n","def standardize_target(df):\n","  keys = df.keys()[3:8]\n","  new_df = pd.DataFrame(std_scaler.fit_transform(df.iloc[:,3:8]), columns = keys)\n","  df.update(new_df)\n","  return \n","standardize_target(df)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oa30U4UpLvS1"},"source":["### FORM TARGETS"]},{"cell_type":"code","metadata":{"id":"vX5dljgA_7C-"},"source":["# FUNCTION TO FORM TARGETS AND OTHER FEATURES IN SEPARATE DATA STRUCTURES\n","\n","def create_glove_embedding(sentence, embedding_dict):\n","  tokens = []\n","  seq_len = len(sentence)\n","  pre_padding = [list(embedding_dict[\"pad\"]) for i in range(max(0, MAX_LEN - seq_len))]\n","  att_mask = [[0, 0, 0, 0, 0] for i in range(MAX_LEN - seq_len)] + [[1, 1, 1, 1, 1] for j in range(seq_len)]\n","  tokens += pre_padding\n","  for word in sentence:\n","    try:\n","      tokens.append(list(embedding_dict[word]))\n","    except:\n","      tokens.append(list(embedding_dict[\"unk\"]))\n","  return tokens, att_mask\n","\n","def form_targets(sentence_tokens, df, MAX_LEN, train = True):\n","  start_pos = 800\n","  if train:\n","    start_pos = 0\n","  n = np.array(df[\"sentence_id\"])[-1]\n","  targets = []\n","  tags = []\n","  embeddings = []\n","  attention_masks = []\n","  features = {\"n_chars\" : [],\n","              \"stopword\" : [],\n","              \"number\" : [],\n","              \"endword\":[],\n","              \"n_char_lemmatized\" : [],\n","              \"tf_idf\" : [],\n","              }\n","\n","  for i in range(start_pos, n+1):\n","    feature = {}\n","    actual_features = {}\n","    temp_df = df[df.sentence_id == i]\n","    sentence_tokens = [w for w in temp_df.word]\n","    \n","    #attention_mask = [0 for j in range(MAX_LEN - len(sentence_tokens))] + [1 for j in range(len(sentence_tokens))]\n","    \n","    if train:\n","      target = [[0, 0, 0, 0, 0] for j in range(MAX_LEN - len(sentence_tokens))]\n","      actual_targets = [list(x) for x in np.array(temp_df.iloc[:, 3:8])]\n","      target += actual_targets\n","    \n","    embedding, att_mask = create_glove_embedding(sentence_tokens, glove_embedding_dict)\n","    attention_masks.append(att_mask)\n","\n","    for key in features.keys():\n","      feature[key] = [0 for j in range(MAX_LEN - len(sentence_tokens))]\n","      actual_features[key] = list(np.array(temp_df.loc[:, key]))\n","      feature[key] += actual_features[key]\n","\n","    tag = [[0 for j in range(7)] for k in range(MAX_LEN - len(sentence_tokens))]\n","    actual_tag = [list(x) for x in np.array(temp_df.iloc[:, -8:-1])]\n","    tag += actual_tag\n","\n","  \n","\n","    for key in features.keys():\n","      features[key].append(feature[key]) \n","    if train:\n","      targets.append(target) \n","    tags.append(tag)\n","    embeddings.append(embedding)\n","\n","  return targets, features, tags, attention_masks, embeddings\n","\n","targets, features, tags, attention_masks, embeddings = form_targets(sentences, df, MAX_LEN)\n","test_targets, test_features, test_tags, test_attention_masks, test_embeddings = form_targets(test_sentences, test_df, MAX_LEN, False)\n","\n","print(\"Target Shape = \", np.array(targets).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoL9zrWUl0Uo"},"source":["print(np.array(features[\"n_chars\"]).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEagKyfcmcnf"},"source":["np.array(attention_masks).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpi13bF3zJGQ"},"source":["np.array(tags).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrYcOS4HBQ-8"},"source":["def create_model_inputs(features, tags):\n","  extra_features = tf.zeros(shape = (np.array(tags).shape[0], 65, 0), dtype = tf.float32)\n","  for key in features.keys():\n","    t = tf.convert_to_tensor(features[key],dtype = tf.float32)\n","    t = tf.expand_dims(t, axis = 2)\n","    extra_features = tf.concat((extra_features, t), axis = 2)\n","  extra_features = tf.concat((extra_features, tags), axis = 2)\n","  return extra_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYDHvncNC0-U"},"source":["extra_features = create_model_inputs(features, tags)\n","test_extra_features = create_model_inputs(test_features, test_tags)\n","extra_features.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQInGJg2gY0Q"},"source":["test_extra_features.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWwo1b9wShvG"},"source":["embeddings = tf.convert_to_tensor(embeddings, dtype = tf.float32)\n","test_embeddings = tf.convert_to_tensor(test_embeddings, dtype = tf.float32)\n","embeddings.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDAzyWM7gUJN"},"source":["test_embeddings.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hSf3TV4Sr7y"},"source":["targets = tf.convert_to_tensor(targets, dtype = tf.float32)\n","targets.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26dmRgA1VxWT"},"source":["np.array(attention_masks).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pO5HEHFAgb69"},"source":["np.array(test_attention_masks).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2_XK7r2Tg7_"},"source":["targets.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTs4imifRrVb"},"source":["### IMPORT TOKENIZERS AND MODELS"]},{"cell_type":"code","metadata":{"id":"vDHVsPObPAdP"},"source":["from sklearn.model_selection import train_test_split\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5SeAo61Vsj1"},"source":["### EARLY STOPPING CLASS"]},{"cell_type":"markdown","metadata":{"id":"VMB7hgQSRyS0"},"source":["### FORMING CLASS OF DATA"]},{"cell_type":"code","metadata":{"id":"SpCysM36L4ux"},"source":["device = \"cpu\"\n","with tf.device(device):\n","  # INPUT LAYER\n","\n","  embedding_input = tf.keras.Input(shape = (MAX_LEN, ), name = \"Embedding_Input\")\n","  features_input = tf.keras.Input(shape = (MAX_LEN, N_FEATURES), name = \"Features_Input\")\n","  attention_mask_input = tf.keras.Input(shape = (MAX_LEN, N_TARGETS), name = \"Attention_mask_Input\")\n","\n","  #PASS THROUGH EMBEDDING\n","  GloVe = tf.keras.layers.Embedding(len(word_index) + 1, 200, weights=[embedding_matrix], input_length=MAX_LEN, trainable=True)\n","  embedding_output = GloVe(embedding_input)\n","\n","  # PASS THROUGH LANGUAGE LSTMS\n","\n","  lang_input_layer = tf.keras.layers.Dense(256, activation = \"relu\", name = \"Lang_Input_layer\")\n","  lang_forward_layer = tf.keras.layers.LSTM(128, return_sequences = True, name = \"GRU_Layer\")\n","  lang_bilayer = tf.keras.layers.Bidirectional(lang_forward_layer, merge_mode = \"concat\", name = \"BiGRU_Layer\")\n","  lang_out_layer = tf.keras.layers.Dense(256, activation = \"relu\", name = \"Lang_Output_Layer\")\n","\n","\n","  lang_input = lang_input_layer(embedding_output)\n","  lang_hidden_output = lang_bilayer(lang_input)\n","  lang_hidden_output = lang_bilayer(lang_hidden_output)\n","  lang_hidden_output = lang_bilayer(lang_hidden_output)\n","  lang_output = lang_out_layer(lang_hidden_output)\n","\n","  # PASS THROUGH FEATURE LSTMS\n","\n","  feature_forward_layer = tf.keras.layers.LSTM(128, return_sequences = True, name = \"Feature_GRU_Layer\")\n","  feature_bilayer = tf.keras.layers.Bidirectional(feature_forward_layer, merge_mode = \"concat\", name = \"Feature_BiGRU_Layer\")\n","  feature_dense_layer = tf.keras.layers.Dense(256, activation = \"relu\", name = \"Feature_Dense_Layer\")\n","  feature_out_layer = tf.keras.layers.Dense(256, activation = \"relu\", name = \"Feature_Output_Layer\")\n","\n","  feature_input = feature_dense_layer(features_input)    \n","  feature_hidden_output = feature_bilayer(feature_input)\n","  feature_hidden_output = feature_bilayer(feature_hidden_output)\n","  feature_output = feature_out_layer(feature_hidden_output)\n","\n","  # TAKE A MEAN\n","  output = (lang_output+feature_output)/2\n","\n","  # PASS THROUGH HEAD LAYER\n","\n","  head_forward_layer = tf.keras.layers.LSTM(128, return_sequences = True, name = \"Head_GRU_Layer\")\n","  head_bilayer = tf.keras.layers.Bidirectional(head_forward_layer, merge_mode = \"concat\", name = \"Head_BiGRU_Layer\")\n","  head_dense = tf.keras.layers.Dense(128, activation = \"relu\", name = \"Head_Dense_Layer\")\n","  head_drop = tf.keras.layers.Dropout(rate = 0.1)\n","  head_out = tf.keras.layers.Dense(5, activation = \"relu\", name = \"Head_FinalOutput_Layer\")\n","\n","  head_hidden_output = head_bilayer(output)\n","  head_hidden_output= head_bilayer(head_hidden_output)\n","  head_output = head_drop(head_dense(head_hidden_output))\n","  output = head_out(head_output)\n","\n","  output = tf.math.multiply(output, attention_mask_input)\n","\n","  model = tf.keras.Model(inputs = [embedding_input, features_input, attention_mask_input], outputs = output, name = \"EyeGazeModel\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A49IfNFsYK2O"},"source":["def custom_r2(pred, true):\n","  ssr = tf.math.reduce_sum(tf.math.square(true - pred))\n","  sst = tf.math.reduce_sum(tf.math.square(true - tf.math.reduce_mean(true)))\n","  a = 1-ssr/sst\n","  return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4x2Zmsnyo1E"},"source":["from sklearn.metrics import r2_score\n","\n","loss_fn = tf.keras.losses.MAE\n","adam = tf.keras.optimizers.Adam(learning_rate = 1e-3, beta_1 = 0.902, beta_2 = 0.999)\n","\n","model.compile(optimizer = adam, loss = loss_fn, metrics = custom_r2)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpzbkkBuhQ6x"},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", min_delta = 0.0001, patience = 8, verbose = 2, restore_best_weights = True)\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='tflstm_best_model.hdf5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n","\n","history = model.fit(x = [pad_sent, extra_features.numpy(), np.array(attention_masks)], y = targets.numpy(), batch_size = 32, shuffle = True, epochs = 100, verbose = 1, validation_split = 0.25, callbacks=[callback, model_checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q813C2xflK2j"},"source":["train_preds = model.predict(x = [pad_sent, extra_features.numpy(), np.array(attention_masks)], batch_size = 1, verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MIEbcB0mSRe"},"source":["print(r2_score(train_preds[:,:,0], targets[:,:,0]))\n","print(r2_score(train_preds[:,:,1], targets[:,:,1]))\n","print(r2_score(train_preds[:,:,2], targets[:,:,2]))\n","print(r2_score(train_preds[:,:,3], targets[:,:,3]))\n","print(r2_score(train_preds[:,:,4], targets[:,:,4]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yB2wotWGeb4F"},"source":["preds = model.predict(x = [test_pad_sent, test_extra_features.numpy(), np.array(test_attention_masks)], batch_size = 1, verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRUkcgNNmSUK"},"source":["plt.plot(history.history[\"loss\"], c = \"g\", label = \"Train Loss\")\n","plt.plot(history.history[\"val_loss\"], c = \"r\", label = \"Val Loss\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxCdkyAqPnSh"},"source":["model.save(filepath = \"/content/drive/My Drive/CMCL Shared Task/TFBiLSTM_DE_Features.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c03S-A7QPnVF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pvLuN1LPnYI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pshm7LzymikU"},"source":["### TRAINING"]},{"cell_type":"code","metadata":{"id":"jEZelBKThQiU"},"source":["from collections import defaultdict\n","\n","history = defaultdict(list)\n","tolerance = 0\n","best = {}\n","best = {\"val_loss\" : 10000}\n","early_stopping = EarlyStopping(patience = 20, verbose = True)\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 120)\n","\n","  train_loss, train_r2 = train_epoch(model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device)\n","  \n","\n","  print(f'Train loss {train_loss} and Train R2 {train_r2}')\n","\n","  val_loss, val_r2 = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device\n","  )\n","\n","  print(f'Val loss {val_loss} and Val R2 {val_r2}')\n","  print()\n","\n","  history = update(history, train_loss, val_loss, train_r2, val_r2)\n","  \n","  if val_loss < best[\"val_loss\"]:\n","    best = save(history, best)\n","  \n","  early_stopping(val_loss, model)\n","  if early_stopping.early_stop:\n","    print(\"Stopped Early at at Epoch \", epoch+1)\n","    break\n","  model.load_state_dict(torch.load('checkpoint.pt'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXq0k8_FuX36"},"source":["plt.plot(history[\"train_loss\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_loss\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"L1 Loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a01UuYhsaqf"},"source":["plt.plot(history[\"train_nFix\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_nFix\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"nFix R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmdo1mvZ2EZu"},"source":["plt.plot(history[\"train_FFD\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_FFD\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"FFD R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJHdVFTP2Elc"},"source":["plt.plot(history[\"train_GPT\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_GPT\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"GPT R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JbTNzVD2E0I"},"source":["plt.plot(history[\"train_TRT\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_TRT\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"TRT R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aQVAHfL2Feb"},"source":["plt.plot(history[\"train_fixProp\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_fixProp\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"fixProp R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dEi49j1uX0k"},"source":["train_arr = np.array([best[\"val_nFix\"], best[\"val_FFD\"], best[\"val_GPT\"], best[\"val_TRT\"], best[\"val_fixProp\"]])\n","val_arr = np.array([best[\"val_nFix\"], best[\"val_FFD\"], best[\"val_GPT\"], best[\"val_TRT\"], best[\"val_fixProp\"]])\n","\n","train_mean = np.mean(train_arr)\n","train_std = np.std(train_arr)\n","train_M = np.max(train_arr)\n","train_m = np.min(train_arr)\n","val_mean = np.mean(val_arr)\n","val_std = np.std(val_arr)\n","val_M = np.max(val_arr)\n","val_m = np.min(val_arr)\n","\n","display_data = [[best[\"train_loss\"], best[\"train_nFix\"], best[\"train_FFD\"], best[\"train_GPT\"], best[\"train_TRT\"], best[\"train_fixProp\"], train_mean, train_std, train_M, train_m],\n"," [best[\"val_loss\"], best[\"val_nFix\"], best[\"val_FFD\"], best[\"val_GPT\"], best[\"val_TRT\"], best[\"val_fixProp\"], val_mean, val_std, val_M, val_m]]\n","\n","display_df = pd.DataFrame(display_data, columns = [\"L1Loss\", \"nFix\", \"FFD\", \"GPT\", \"TRT\", \"fixProp\", \"Mean\", \"Std Deviation\", \"Max\", \"Min\"], index = [\"Train\", \"Val\"]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNm1xrc-as7a"},"source":["display_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qbg7_c4A0PGi"},"source":["display_data2 = [[history[\"train_loss\"][-1], history[\"train_nFix\"][-1], history[\"train_FFD\"][-1], history[\"train_GPT\"][-1], history[\"train_TRT\"][-1], history[\"train_fixProp\"][-1]], \n","                 [history[\"val_loss\"][-1], history[\"val_nFix\"][-1], history[\"val_FFD\"][-1], history[\"val_GPT\"][-1], history[\"val_TRT\"][-1], history[\"val_fixProp\"][-1]]]\n","display_df2 = pd.DataFrame(display_data2, columns = [\"L1Loss\", \"nFix\", \"FFD\", \"GPT\", \"TRT\", \"fixProp\"], index = [\"Train \", \"Val \"]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSMzdfBm07XO"},"source":["display_df2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1i0WDWv94G1"},"source":["history[\"train_loss\"][0], history[\"val_loss\"][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGajW6WtuNU6"},"source":["save_file_path = \"/content/drive/My Drive/CMCL Shared Task/LSTM_Features_20.pth\"\n","torch.save(model.state_dict(), save_file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7z_rq5uWNOz"},"source":["del model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1L816924MjQC"},"source":["'''\n","IDEAS:\n","Training :-\n","1. Sentence Formation.\n","2. BERT tokenize.\n","3. Base BERT Model -> if encoded value == 0 -> 5 output Dense Layer.\n","4. MAE metric for loss calculation.\n","\n","Test :-\n","1. Sentence Formation.\n","2. BERT Tokenize.\n","3. Base BERT Model  -> if encoded value == 0 -> 5 output Dense Layer.\n","4. Order is maintained and predictions are pasted on the csv file.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_Vkv6vi5UdI"},"source":["'''\n","trainer = Engine(train_epoch)\n","train_evaluator = Engine(train_epoch)\n","validation_evaluator = Engine(val_epoch)\n","\n","Loss(loss_fn).attach(train_evaluator, \"l1\")\n","Loss(loss_fn).attach(validation_evaluator, \"l1\")\n","\n","def score_function(engine):\n","    val_loss = engine.state.metrics['nll']\n","    return val_loss\n","\n","handler = EarlyStopping(patience = 10, score_function=score_function, trainer = trainer)\n","validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n","\n","def log_training_results(engine):\n","    train_evaluator.run(train_data_loader)\n","    metrics = train_evaluator.state.metrics\n","    pbar.log_message(\n","        \"Training Results - Epoch: {} \\nMetrics\\n{}\"\n","        .format(engine.state.epoch, pprint.pformat(metrics)))\n","    \n","def log_validation_results(engine):\n","    validation_evaluator.run(val_data_loader)\n","    metrics = validation_evaluator.state.metrics\n","    metrics = validation_evaluator.state.metrics\n","    pbar.log_message(\n","        \"Validation Results - Epoch: {} \\nMetrics\\n{}\"\n","        .format(engine.state.epoch, pprint.pformat(metrics)))\n","    pbar.n = pbar.last_print_n = 0\n","\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n","\n","checkpointer = ModelCheckpoint('checkpoint', 'textcnn', save_interval=1, n_saved=2, create_dir=True, save_as_state_dict=True)\n","\n","best_model_save = ModelCheckpoint(\n","    'best_model', 'textcnn', n_saved=1,\n","    create_dir=True, save_as_state_dict=True,\n","    score_function=score_function)\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'textcnn': model})\n","validation_evaluator.add_event_handler(Events.EPOCH_COMPLETED, best_model_save, {'textcnn': model})\n","\n","\n","trainer.run(train_data_loader, max_epochs=120)\n","'''"],"execution_count":null,"outputs":[]}]}