{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EyeGazeSharedTask_Features+LSTM.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"80bHKBp1j9ae"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyBe-0ddKQaE"},"source":["# EYE GAZE SHARED TASK"]},{"cell_type":"code","metadata":{"id":"e8yEHuC4J9S1"},"source":["import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import r2_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOMNpLleKSkN"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rtPkvPN0kJPV"},"source":["### DOWNLOAD DATA"]},{"cell_type":"code","metadata":{"id":"kRxJ5AXBLUy5"},"source":["train_data_file_path = \"/content/drive/My Drive/CMCL Shared Task/training_data.csv\"\n","#file_path = \"/content/drive/MyDrive/datasets/EyeGazeSharedTask/trial_data.csv\"\n","\n","train_data = pd.read_csv(train_data_file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZYNfiti9c-D"},"source":["print(\"Train Data Shape = \", train_data.shape)\n","train_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Ude7qQ6hZxM"},"source":["### BASIC MODEL PLAN"]},{"cell_type":"code","metadata":{"id":"1a6iBuw2Zi3a","cellView":"form"},"source":["#@title\n","import numpy as np\n","file_path = \"/content/drive/My Drive/CMCL Shared Task/glove.6B.200d.txt\"\n","\n","def loadGloveModel(File):\n","    print(\"Loading Glove Model\")\n","    f = open(File,'r')\n","    gloveModel = {}\n","    for line in f:\n","        splitLines = line.split()\n","        word = splitLines[0]\n","        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n","        gloveModel[word] = wordEmbedding\n","    print(len(gloveModel),\" words loaded!\")\n","    return gloveModel\n","\n","glove_embedding_dict = loadGloveModel(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbjD1vIq2zno"},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### PREPROCESS DATA"]},{"cell_type":"markdown","metadata":{"id":"fYLOLvUm1jOD"},"source":["#### REMOVE EOS TOKEN"]},{"cell_type":"code","metadata":{"id":"J1P_bWCs2baB"},"source":["#@title\n","# FUNCTION TO REMOVE THE <EOS> TOKEN IN THE DATASET\n","\n","def remove_eos(df):\n","  cnt = 1\n","  for i in range(df.shape[0]-1):\n","    if (df.loc[i+1, \"sentence_id\"] == cnt):\n","      df.loc[i, \"word\"] = df.loc[i, \"word\"][:-5]   # Remove <EOS> for the last word of each sentence.\n","      cnt += 1 \n","  s = df.loc[df.shape[0] - 1, \"word\"]              # Remove <EOS> for last element separately\n","  df.loc[df.shape[0] - 1, \"word\"] = s[:-6] \n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0qirQb-2EE7"},"source":["### NO OF CHARS FOR EACH WORD"]},{"cell_type":"code","metadata":{"id":"JQ4D7kRN1cOB"},"source":["#@title\n","# FUNCTION TO CALCULATE THE NUMBER OF CHARACTERS PER WORD\n","# ADDS THE DATA IN A NEW COLUMN\n","\n","def char_per_word(df):\n","  n_chars = []\n","  for word in df.word:\n","    n_chars.append(len(str(word)))\n","  df[\"n_chars\"] = n_chars\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJnVAg8QccsI"},"source":["### NO OF CHARS OF WORD - LEMMATIZED WORD"]},{"cell_type":"code","metadata":{"id":"6nVwk1nab9nf"},"source":["#@title\n","# FUNCTION TO CALCULATE THE DIFFERENCE BETWEEN NUMBER OF CHARACTERS IN WORD AND LEMMATIZED WORD\n","# ADDS AS A NEW COLUMN\n","\n","from nltk import WordNetLemmatizer\n","\n","Lemmatizer = WordNetLemmatizer()\n","\n","def char_per_lemmatized_word(df):\n","  n_chars = []\n","  for word in df.word:\n","    n_chars.append(len(str(word)) - len(Lemmatizer.lemmatize(word)))\n","  df[\"n_char_lemmatized\"] = n_chars\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNya6DDO2KlQ"},"source":["### STOP WORD OR NOT"]},{"cell_type":"code","metadata":{"id":"P9wvuFxh1fKX"},"source":["#@title\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","\n","stopwords = nltk.corpus.stopwords\n","stop_words = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvOW7IN72NJy"},"source":["#@title\n","# FUNCTION TO ASSERT WHETHER A WORD IS STOPWORD  OR NOT\n","# ADDS THE DATA IN A NEW COLUMN\n","\n","def add_stopword_check(df):\n","  if_stopword = []\n","  for word in df.word:\n","    if_stopword.append(int(word in stop_words))\n","\n","  df[\"stopword\"] = if_stopword\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzMv3ir42Wl4"},"source":["### NUMBER OR NOT"]},{"cell_type":"code","metadata":{"id":"6knkuEi12RAb"},"source":["#@title\n","# FUNCTION TO DEFINE WHETHER IT IS A NUMBER OR NOT\n","# ADDS THE DATA AS A NEW COLUMN\n","\n","def add_number_check(df):\n","  if_number = []\n","  for word in df.word:\n","    if_number.append(int(word.isdigit()))\n","\n","  df[\"number\"] = if_number\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2AGt92OXu81b"},"source":["### TF IDF CALCULATION"]},{"cell_type":"code","metadata":{"id":"b_v29e6ccqag"},"source":["#@title\n","# FUNCTION TO CALCULATE THE TFIDF OF THE TRAINING DATASET\n","# ADDS THE DATA IN A NEW COLUMN\n","# ALSO RETURNS A LIST OF THE SENTENCES\n","\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(stop_words=None)\n","bad_words = []\n","punc = string.punctuation\n","\n","def remove_punc(word):\n","  table = str.maketrans('', '', punc)\n","  return word.translate(table)\n","\n","def calc_tfidf(df):\n","  n = np.array(df[\"sentence_id\"])[-1]\n","  sentences = []\n","  sentence_tokens = []\n","  tf_idfs = []\n","  MAX_LEN = 0\n","  for i in range(n+1):   \n","    temp_df = df[df.sentence_id == i]\n","    sentence = (' ').join(temp_df.word)\n","    MAX_LEN = max(MAX_LEN, len(sentence))\n","    sentences.append(sentence)\n","    sentence_tokens.append([np.array(temp_df.word)])\n","  tf_idf = vectorizer.fit_transform(sentences)\n","  for i, word in enumerate(df.word):\n","    try:\n","      tf_idfs.append(tf_idf.toarray()[df[\"sentence_id\"][i]][vectorizer.get_feature_names().index(remove_punc(word.lower()))])\n","    except:\n","      bad_words.append(word)\n","      if word in [\"a\", \"A\"]:\n","        tf_idfs.append(0.8)\n","      else:\n","        tf_idfs.append(0.01)\n","  df[\"tf_idf\"] = tf_idfs\n","\n","\n","  return sentence_tokens, df, bad_words, MAX_LEN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uODThGOIrKDA","cellView":"form"},"source":["#@title\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize \n","from sklearn.preprocessing import OneHotEncoder\n","\n","enc = OneHotEncoder(sparse = False)\n","\n","def pos_tag_func(df):\n","  tags = []\n","  for word in df.word:\n","    if word not in string.punctuation:\n","      tag = nltk.pos_tag(word)[0][1]\n","    else:\n","      tag = \"PUNC\"\n","    tags.append(tag)\n","  df[\"tags\"] = tags\n","  tag_transform = pd.DataFrame(enc.fit_transform(np.array(df.tags).reshape(-1, 1)))\n","  df = pd.concat((df, tag_transform), axis = 1)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8IEDwE_rv5i","cellView":"form"},"source":["#@title\n","def use_transformed_GPT(df):\n","  df[\"GPT\"] = df[\"TRT\"] - df[\"GPT\"]\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zww9QlLhxHfw"},"source":["# FUNCTION TO PERFORM ALL PREPROCESSING STEPS\n","\n","def preprocess(df):\n","  return calc_tfidf(use_transformed_GPT(pos_tag_func(add_number_check(add_stopword_check(char_per_lemmatized_word(char_per_word(remove_eos(df))))))))\n","\n","sentence_tokens, df, bad_words, MAX_LEN = preprocess(train_data)\n","\n","df1 = df.copy()\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7uWHqgmL8vm"},"source":["print(\"No of words unaccounted for = \", len(bad_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kq9lKYwYoLkv"},"source":["# FUNCTION TO NORMALISE ALL THE TARGET VALUES\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","std_scaler = StandardScaler()\n","\n","def standardize_target(df):\n","  keys = df.keys()[3:8]\n","  new_df = pd.DataFrame(std_scaler.fit_transform(df.iloc[:,3:8]), columns = keys)\n","  df.update(new_df)\n","  return \n","standardize_target(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaSNMVstMDmu"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oa30U4UpLvS1"},"source":["### FORM TARGETS"]},{"cell_type":"code","metadata":{"id":"vX5dljgA_7C-"},"source":["# FUNCTION TO FORM TARGETS AND OTHER FEATURES IN SEPARATE DATA STRUCTURES\n","\n","def form_targets(sentence_tokens, df, MAX_LEN):\n","  n = np.array(df[\"sentence_id\"])[-1]\n","  targets = []\n","  tags = []\n","  features = {\"n_chars\" : [],\n","              \"stopword\" : [],\n","              \"number\" : [],\n","              \"n_char_lemmatized\" : [],\n","              \"tf_idf\" : [],\n","              }\n","\n","  for i in range(n+1):\n","    feature = {}\n","    actual_features = {}\n","    temp_df = df[df.sentence_id == i]\n","    \n","    target = [[0, 0, 0, 0, 0] for i in range(MAX_LEN - len(sentence_tokens[i][0]))]\n","    actual_targets = [list(x) for x in np.array(temp_df.iloc[:, 3:8])]\n","    target += actual_targets\n","\n","    for key in features.keys():\n","      feature[key] = [0 for j in range(MAX_LEN - len(sentence_tokens[i][0]))]\n","      actual_features[key] = list(np.array(temp_df.loc[:, key]))\n","      feature[key] += actual_features[key]\n","\n","    tag = [[0 for j in range(20)] for k in range(MAX_LEN - len(sentence_tokens[i][0]))]\n","    actual_tag = [list(x) for x in np.array(temp_df.iloc[:, -20:])]\n","    tag += actual_tag\n","\n","    for key in features.keys():\n","      features[key].append(feature[key]) \n","    targets.append(target) \n","    tags.append(tag)\n","\n","  return targets, features, tags\n","\n","targets, features, tags = form_targets(sentence_tokens, df, MAX_LEN)\n","print(\"Target Shape = \", np.array(targets, dtype = \"object\").shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTs4imifRrVb"},"source":["### IMPORT TOKENIZERS AND MODELS"]},{"cell_type":"code","metadata":{"id":"vDHVsPObPAdP"},"source":["from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5SeAo61Vsj1"},"source":["### EARLY STOPPING CLASS"]},{"cell_type":"code","metadata":{"id":"VaAhnikZiHdu","cellView":"form"},"source":["#@title\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=20, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMB7hgQSRyS0"},"source":["### FORMING CLASS OF DATA"]},{"cell_type":"code","metadata":{"id":"f1UeHlE-OD19"},"source":["# Class for the data\n","# Contains 4 keys -> Sentences, targets, input_ids(for BERT), attention_mask(for BERT) \n","\n","class EyeGaze_dataset(Dataset):\n","  def __init__(self, sentence_tokens, targets, features, tags, MAX_LEN):\n","    self.sentences = sentence_tokens               # List of Sentences\n","    self.max_len = MAX_LEN\n","    self.targets = targets                   # List of Padded Targets\n","    self.features = features\n","    self.tags = tags\n","    \n","  def __len__(self):\n","    return len(self.sentences)               # No of Examples = 100\n","\n","  def create_glove_embedding(self, sentence, embedding_dict):\n","    tokens = []\n","    seq_len = len(sentence)\n","    pre_padding = [list(embedding_dict[\"pad\"]) for i in range(max(0, self.max_len - seq_len))]\n","    tokens += pre_padding\n","    for word in sentence:\n","      try:\n","        tokens.append(list(embedding_dict[word]))\n","      except:\n","        tokens.append(list(embedding_dict[\"unk\"]))\n","    return tokens, pre_padding\n","\n","  def __getitem__(self, index):\n","    sentence = list(self.sentences[index][0])        # Get the review at the particular index\n","    target = self.targets[index]                    # Get the target label at the particular index\n","    embedding, pre_padding = self.create_glove_embedding(sentence, glove_embedding_dict)\n","    feature = {}\n","    for key in features.keys():\n","      feature[key] = features[key][index]\n","    tag = tags[index]\n","    \n","    # The class simply returns a dictionary of the following\n","    return_dict = {\"sentence\" : sentence,\n","                   \"target\":torch.tensor(target, dtype = torch.float32),\n","                   \"embedding\" : torch.tensor(embedding, dtype = torch.float32),\n","                   \"padding\": torch.tensor(pre_padding, dtype = torch.long),\n","                   \"tag\":torch.tensor(tag, dtype = torch.float32)}\n","    for key in features.keys():\n","      return_dict[key] = torch.tensor(feature[key], dtype = torch.float32)\n","\n","    return return_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9UZXPJ7PnLf"},"source":["# Forms the DataLoaders with test_size = 0.2\n","# Each DataLoader has the Class dataset as their elements\n","def create_data_loader(sentences, targets, features, tags, MAX_LEN, batch_size):\n","  data = EyeGaze_dataset(sentences, targets, features, tags, MAX_LEN)\n","  train, val = train_test_split(data, test_size = 0.2, shuffle = False)\n","  train_data_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n","  val_data_loader = DataLoader(val, batch_size = batch_size, shuffle = False)\n","  \n","  return train_data_loader, val_data_loader\n","\n","BATCH_SIZE = 1\n","train_data_loader, val_data_loader = create_data_loader(sentence_tokens, targets, features, tags, MAX_LEN, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWaObK0LqAHv"},"source":["temp = next(iter(train_data_loader))\n","print(temp.keys())\n","keys = ['target', 'embedding', \"padding\", 'n_chars', 'n_char_lemmatized', 'stopword', 'number', 'tf_idf', \"tag\"]\n","for key in keys:\n","  print(temp[key].size())\n","print(temp['sentence'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eg0IgueMNcMz"},"source":["Sentences Shape = _(100, variable)_\n","\n","Tokens Shape = _(100, 320)_\n","\n","Hidden Shape = _(100, 320, 768)_\n","\n","Final Output Shape = _(100, 320, 5)_\n","\n","Target Shape = _(100, 320, 5)_"]},{"cell_type":"code","metadata":{"id":"CVWMfJQr-ybR"},"source":["class FeatureSideModel(torch.nn.Module):\n","  def __init__(self):\n","    super(FeatureSideModel, self).__init__()\n","    self.encoder = torch.nn.LSTM(input_size = 25, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.1, bidirectional = True)\n","    \n","  def forward(self, features):\n","    output, h = self.encoder(features)\n","     \n","    return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpCysM36L4ux"},"source":["class EyeGazeClassifier(torch.nn.Module):\n","  def __init__(self):\n","    super(EyeGazeClassifier, self).__init__()\n","    self.feature_model = FeatureSideModel()\n","    self.encoder = torch.nn.LSTM(input_size = 200, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.1, bidirectional = True)\n","    self.dense1 = torch.nn.Linear(in_features = 512, out_features = 128)\n","    self.act = torch.nn.ReLU()\n","    self.drop = torch.nn.Dropout(p = 0.1)\n","    self.dense2 = torch.nn.Linear(in_features = 128, out_features = 5)\n","\n","  def forward(self, tokens, attention_mask, features):\n","    feature_output = self.feature_model(features)\n","    output, h = self.encoder(tokens)\n","    output = (output+feature_output)/2\n","    output = self.drop(self.act(self.dense1(output)))\n","    output = self.dense2(output)\n","    output = output * attention_mask\n","\n","    return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4x2Zmsnyo1E"},"source":["#del model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpzbkkBuhQ6x"},"source":["model = EyeGazeClassifier()\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pshm7LzymikU"},"source":["### TRAINING"]},{"cell_type":"code","metadata":{"id":"kMivdfe8mhwj"},"source":["#@title\n","EPOCHS = 150\n","#from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=8e-6, betas = (0.9, 0.999))\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","#scheduler = get_linear_schedule_with_warmup(\n"," # optimizer,\n"," # num_warmup_steps=6,\n"," # num_training_steps=total_steps\n","#)\n","\n","loss_fn = nn.L1Loss(reduction = \"sum\").to(device)  # reduction = \"mean\" can be used"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgQwPYEV0pO-","cellView":"form"},"source":["#@title\n","def target_initialise():\n","  targets = {}\n","  outputs = {}\n","\n","  targets[\"r20\"] = torch.tensor([], dtype = torch.float32)\n","  targets[\"r21\"] = torch.tensor([], dtype = torch.float32)\n","  targets[\"r22\"] = torch.tensor([], dtype = torch.float32)\n","  targets[\"r23\"] = torch.tensor([], dtype = torch.float32)\n","  targets[\"r24\"] = torch.tensor([], dtype = torch.float32)\n","\n","  outputs[\"pred_r20\"] = torch.tensor([], dtype = torch.float32)\n","  outputs[\"pred_r21\"] = torch.tensor([], dtype = torch.float32)\n","  outputs[\"pred_r22\"] = torch.tensor([], dtype = torch.float32)\n","  outputs[\"pred_r23\"] = torch.tensor([], dtype = torch.float32)\n","  outputs[\"pred_r24\"] = torch.tensor([], dtype = torch.float32)\n","  \n","  return targets, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJl4qA4NAziZ","cellView":"form"},"source":["#@title\n","def store_targets(targets, target):\n","  targets[\"r20\"] = torch.cat([targets[\"r20\"], target[0,:,0].detach().cpu()], dim = 0)\n","  targets[\"r21\"] = torch.cat([targets[\"r21\"], target[0,:,1].detach().cpu()], dim = 0)\n","  targets[\"r22\"] = torch.cat([targets[\"r22\"], target[0,:,2].detach().cpu()], dim = 0)\n","  targets[\"r23\"] = torch.cat([targets[\"r23\"], target[0,:,3].detach().cpu()], dim = 0)\n","  targets[\"r24\"] = torch.cat([targets[\"r24\"], target[0,:,4].detach().cpu()], dim = 0)\n","\n","  return targets  \n","\n","def store_outputs(outputs, output):\n","  outputs[\"pred_r20\"] = torch.cat([outputs[\"pred_r20\"], output[0,:,0].detach().cpu()], dim = 0)\n","  outputs[\"pred_r21\"] = torch.cat([outputs[\"pred_r21\"], output[0,:,1].detach().cpu()], dim = 0)\n","  outputs[\"pred_r22\"] = torch.cat([outputs[\"pred_r22\"], output[0,:,2].detach().cpu()], dim = 0)\n","  outputs[\"pred_r23\"] = torch.cat([outputs[\"pred_r23\"], output[0,:,3].detach().cpu()], dim = 0)\n","  outputs[\"pred_r24\"] = torch.cat([outputs[\"pred_r24\"], output[0,:,4].detach().cpu()], dim = 0)\n","  \n","  return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAgYweVamhtT"},"source":["#@title\n","def train_epoch(model, data_loader, loss_fn, optimizer, device):\n","  model = model.train()\n","\n","  losses = []\n","  targets, outputs = target_initialise()  \n","  \n","  r2 = {}\n","  \n","  for d in data_loader:\n","    tokens = d[\"embedding\"].to(device)\n","    target = d[\"target\"].to(device)\n","    char_len = torch.unsqueeze(d[\"n_chars\"], dim = 2)\n","    char_len_lemmatized = torch.unsqueeze(d[\"n_char_lemmatized\"], dim = 2)\n","    if_stopword = torch.unsqueeze(d[\"stopword\"], dim = 2)\n","    if_num = torch.unsqueeze(d['number'], dim = 2)\n","    tfidf = torch.unsqueeze(d[\"tf_idf\"], dim = 2)\n","    tag = d[\"tag\"]\n","\n","    targets = store_targets(targets, target)\n","    extra_feature = torch.cat((char_len, char_len_lemmatized, if_stopword, if_num, tfidf, tag), dim = 2).to(device)\n","    \n","    attention_mask = [0 for i in range(d[\"padding\"].size()[1])] + [1 for i in range(MAX_LEN - d[\"padding\"].size()[1])]\n","    attention_mask = torch.unsqueeze(torch.unsqueeze(torch.tensor(attention_mask), dim = 0), dim = 2).to(device)\n","    \n","    output = model(\n","        tokens = tokens, \n","        attention_mask = attention_mask,\n","        features = extra_feature\n","      )\n","\n","    outputs = store_outputs(outputs, output)\n","    \n","    loss = loss_fn(output, target)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    #scheduler.step()\n","    optimizer.zero_grad()\n","  \n","  r2[\"nFix\"] = r2_score(targets[\"r20\"].numpy(), outputs[\"pred_r20\"].numpy())\n","  r2[\"FFD\"] = r2_score(targets[\"r21\"].numpy(), outputs[\"pred_r21\"].numpy())\n","  r2[\"GPT\"] = r2_score(targets[\"r22\"].numpy(), outputs[\"pred_r22\"].numpy())\n","  r2[\"TRT\"] = r2_score(targets[\"r23\"].numpy(), outputs[\"pred_r23\"].numpy())\n","  r2[\"fixProp\"] = r2_score(targets[\"r24\"].numpy(), outputs[\"pred_r24\"].numpy())\n","  \n","  return np.mean(losses), r2    # correct_predictions.double() / n_examples,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uScvRCt2mhqV"},"source":["\n","#@title\n","def eval_model(model, data_loader, loss_fn, device):\n","  model = model.eval()\n","\n","  losses = []\n","  outputs = []\n","  r2 = {}\n","  targets, outputs = target_initialise()  \n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      tokens = d[\"embedding\"].to(device)\n","      target = d[\"target\"].to(device)\n","      char_len = torch.unsqueeze(d[\"n_chars\"], dim = 2)\n","      char_len_lemmatized = torch.unsqueeze(d[\"n_char_lemmatized\"], dim = 2)\n","      if_stopword = torch.unsqueeze(d[\"stopword\"], dim = 2)\n","      if_num = torch.unsqueeze(d['number'], dim = 2)\n","      tfidf = torch.unsqueeze(d[\"tf_idf\"], dim = 2)\n","      tag = d[\"tag\"]\n","\n","      targets = store_targets(targets, target)\n","      extra_feature = torch.cat((char_len, char_len_lemmatized, if_stopword, if_num, tfidf, tag), dim = 2).to(device)\n","      \n","      attention_mask = [0 for i in range(d[\"padding\"].size()[1])] + [1 for i in range(MAX_LEN - d[\"padding\"].size()[1])]\n","      attention_mask = torch.unsqueeze(torch.unsqueeze(torch.tensor(attention_mask), dim = 0), dim = 2).to(device)\n","      \n","      output = model(\n","        tokens = tokens, \n","        attention_mask = attention_mask,\n","        features = extra_feature\n","      )\n","      \n","      outputs = store_outputs(outputs, output)\n","    \n","      loss = loss_fn(output, target)\n","      losses.append(loss.item())\n","\n","  r2[\"nFix\"] = r2_score(targets[\"r20\"].numpy(), outputs[\"pred_r20\"].numpy())\n","  r2[\"FFD\"] = r2_score(targets[\"r21\"].numpy(), outputs[\"pred_r21\"].numpy())\n","  r2[\"GPT\"] = r2_score(targets[\"r22\"].numpy(), outputs[\"pred_r22\"].numpy())\n","  r2[\"TRT\"] = r2_score(targets[\"r23\"].numpy(), outputs[\"pred_r23\"].numpy())\n","  r2[\"fixProp\"] = r2_score(targets[\"r24\"].numpy(), outputs[\"pred_r24\"].numpy())\n","  \n","  return np.mean(losses), r2    # correct_predictions.double() / n_examples,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLDMQGO9t-60","cellView":"form"},"source":["#@title\n","def save(history, best):\n","  for key, value in history.items():\n","    best[key] = value[-1]\n","\n","  return best\n","\n","def update(history, train_loss, val_loss, train_r2, val_r2):\n","  \n","  history['train_loss'].append(train_loss)\n","  history['val_loss'].append(val_loss)\n","\n","  history[\"train_nFix\"].append(train_r2[\"nFix\"])\n","  history[\"train_FFD\"].append(train_r2[\"FFD\"])\n","  history[\"train_GPT\"].append(train_r2[\"GPT\"])\n","  history[\"train_TRT\"].append(train_r2[\"TRT\"])\n","  history[\"train_fixProp\"].append(train_r2[\"fixProp\"])\n","\n","  history[\"val_nFix\"].append(val_r2[\"nFix\"])\n","  history[\"val_FFD\"].append(val_r2[\"FFD\"])\n","  history[\"val_GPT\"].append(val_r2[\"GPT\"])\n","  history[\"val_TRT\"].append(val_r2[\"TRT\"])\n","  history[\"val_fixProp\"].append(val_r2[\"fixProp\"])\n","\n","  return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEZelBKThQiU"},"source":["from collections import defaultdict\n","\n","history = defaultdict(list)\n","tolerance = 0\n","best = {}\n","best = {\"val_loss\" : 10000}\n","early_stopping = EarlyStopping(patience = 20, verbose = True)\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 120)\n","\n","  train_loss, train_r2 = train_epoch(model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device)\n","  \n","\n","  print(f'Train loss {train_loss} and Train R2 {train_r2}')\n","\n","  val_loss, val_r2 = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device\n","  )\n","\n","  print(f'Val loss {val_loss} and Val R2 {val_r2}')\n","  print()\n","\n","  history = update(history, train_loss, val_loss, train_r2, val_r2)\n","  \n","  if val_loss < best[\"val_loss\"]:\n","    best = save(history, best)\n","  \n","  early_stopping(val_loss, model)\n","  if early_stopping.early_stop:\n","    print(\"Stopped Early at at Epoch \", epoch+1)\n","    break\n","  model.load_state_dict(torch.load('checkpoint.pt'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXq0k8_FuX36"},"source":["plt.plot(history[\"train_loss\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_loss\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"L1 Loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a01UuYhsaqf"},"source":["plt.plot(history[\"train_nFix\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_nFix\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"nFix R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmdo1mvZ2EZu"},"source":["plt.plot(history[\"train_FFD\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_FFD\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"FFD R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJHdVFTP2Elc"},"source":["plt.plot(history[\"train_GPT\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_GPT\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"GPT R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JbTNzVD2E0I"},"source":["plt.plot(history[\"train_TRT\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_TRT\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"TRT R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aQVAHfL2Feb"},"source":["plt.plot(history[\"train_fixProp\"], c = \"r\", label = \"Train Loss\")\n","plt.plot(history[\"val_fixProp\"], c = \"g\", label = \"Validation Loss\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"fixProp R2\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dEi49j1uX0k"},"source":["display_data = [[best[\"train_loss\"], best[\"train_nFix\"], best[\"train_FFD\"], best[\"train_GPT\"], best[\"train_TRT\"], best[\"train_fixProp\"]],\n"," [best[\"val_loss\"], best[\"val_nFix\"], best[\"val_FFD\"], best[\"val_GPT\"], best[\"val_TRT\"], best[\"val_fixProp\"]]]\n","\n","display_df = pd.DataFrame(display_data, columns = [\"L1Loss\", \"nFix\", \"FFD\", \"GPT\", \"TRT\", \"fixProp\"], index = [\"Train\", \"Val\"]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNm1xrc-as7a"},"source":["display_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qbg7_c4A0PGi"},"source":["display_data2 = [[history[\"train_loss\"][-1], history[\"train_nFix\"][-1], history[\"train_FFD\"][-1], history[\"train_GPT\"][-1], history[\"train_TRT\"][-1], history[\"train_fixProp\"][-1]], \n","                 [history[\"val_loss\"][-1], history[\"val_nFix\"][-1], history[\"val_FFD\"][-1], history[\"val_GPT\"][-1], history[\"val_TRT\"][-1], history[\"val_fixProp\"][-1]]]\n","display_df2 = pd.DataFrame(display_data2, columns = [\"L1Loss\", \"nFix\", \"FFD\", \"GPT\", \"TRT\", \"fixProp\"], index = [\"Train \", \"Val \"]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSMzdfBm07XO"},"source":["display_df2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1i0WDWv94G1"},"source":["history[\"train_loss\"][0], history[\"val_loss\"][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGajW6WtuNU6"},"source":["save_file_path = \"/content/drive/My Drive/CMCL Shared Task/RobertaTokenFeaturesAttentionPosTag.pth\"\n","torch.save(model.state_dict(), save_file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7z_rq5uWNOz"},"source":["del model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1L816924MjQC"},"source":["'''\n","IDEAS:\n","Training :-\n","1. Sentence Formation.\n","2. BERT tokenize.\n","3. Base BERT Model -> if encoded value == 0 -> 5 output Dense Layer.\n","4. MAE metric for loss calculation.\n","\n","Test :-\n","1. Sentence Formation.\n","2. BERT Tokenize.\n","3. Base BERT Model  -> if encoded value == 0 -> 5 output Dense Layer.\n","4. Order is maintained and predictions are pasted on the csv file.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_Vkv6vi5UdI"},"source":["'''\n","trainer = Engine(train_epoch)\n","train_evaluator = Engine(train_epoch)\n","validation_evaluator = Engine(val_epoch)\n","\n","Loss(loss_fn).attach(train_evaluator, \"l1\")\n","Loss(loss_fn).attach(validation_evaluator, \"l1\")\n","\n","def score_function(engine):\n","    val_loss = engine.state.metrics['nll']\n","    return val_loss\n","\n","handler = EarlyStopping(patience = 10, score_function=score_function, trainer = trainer)\n","validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n","\n","def log_training_results(engine):\n","    train_evaluator.run(train_data_loader)\n","    metrics = train_evaluator.state.metrics\n","    pbar.log_message(\n","        \"Training Results - Epoch: {} \\nMetrics\\n{}\"\n","        .format(engine.state.epoch, pprint.pformat(metrics)))\n","    \n","def log_validation_results(engine):\n","    validation_evaluator.run(val_data_loader)\n","    metrics = validation_evaluator.state.metrics\n","    metrics = validation_evaluator.state.metrics\n","    pbar.log_message(\n","        \"Validation Results - Epoch: {} \\nMetrics\\n{}\"\n","        .format(engine.state.epoch, pprint.pformat(metrics)))\n","    pbar.n = pbar.last_print_n = 0\n","\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n","\n","checkpointer = ModelCheckpoint('checkpoint', 'textcnn', save_interval=1, n_saved=2, create_dir=True, save_as_state_dict=True)\n","\n","best_model_save = ModelCheckpoint(\n","    'best_model', 'textcnn', n_saved=1,\n","    create_dir=True, save_as_state_dict=True,\n","    score_function=score_function)\n","trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'textcnn': model})\n","validation_evaluator.add_event_handler(Events.EPOCH_COMPLETED, best_model_save, {'textcnn': model})\n","\n","\n","trainer.run(train_data_loader, max_epochs=120)\n","'''"],"execution_count":null,"outputs":[]}]}